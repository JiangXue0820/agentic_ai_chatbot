# ğŸ§  Agentic AI â€” Secure Mode + Document Upload + Source-Aware Retrieval

## ğŸ“Œ Overview

This feature upgrade introduces **secure query control**, **knowledge ingestion via document upload**, and **source-aware retrieval** for the Agentic AI MVP.

The goal is to make the system safer, smarter, and more explainable â€” all while minimizing changes to the existing architecture.

---

## ğŸ¯ Feature Summary

| Feature | Description |
|----------|--------------|
| **Secure Mode Toggle** | Allow users to enable/disable guardrails (inbound & outbound protection) in UI |
| **Document Upload (PDF/DOCX/TXT/MD)** | Users can upload files which will be automatically parsed, chunked, embedded, and stored in VectorDB |
| **Cross-session Knowledge Retrieval** | All uploaded documents are globally retrievable across sessions |
| **Source-aware Responses** | When knowledge is retrieved from VectorDB, Agent replies must include file name and page number as citations |
| **Separation of Responsibility** | Parsing, chunking, and ingestion logic reside **entirely in backend** (not UI) |

---

## ğŸ§© Module & File Impact Summary

| Module | File Path | Description |
|---------|------------|-------------|
| ğŸ–¥ï¸ **Frontend UI** | `ui_app.py` | Add Secure Toggle + Upload Doc buttons; send file to backend `/tools/vdb/ingest`; display sources in chat |
| âš™ï¸ **Backend API** | `app/api/tools.py` | Add `/vdb/ingest` endpoint (accepts file upload); call VDBAdapter to ingest file |
| ğŸ§  **VectorDB Adapter** | `app/tools/vdb.py` | Implement `ingest_file()` for parsing, chunking, and embedding; attach metadata |
| ğŸ§± **Text Splitter** | `app/utils/text_splitter.py` *(new)* | Split text into chunks with overlap |
| ğŸ“„ **File Parser** | `app/utils/file_parser.py` *(new)* | Extract raw text from PDF, DOCX, MD, and TXT |
| ğŸ’¬ **Agent Core** | `app/agent/core.py` | Append document source info (`filename`, `page`) in Agentâ€™s reply as citations |

---

## âš™ï¸ Implementation Details

### 1ï¸âƒ£ UI â€” Secure Toggle + Upload Doc

**File:** `ui_app.py`

**Changes:**
- Add a horizontal control bar containing:
  - **Secure Mode Toggle**
  - **Upload Doc Button**
  - **Send**
  - **Clear**
- Uploaded files are sent directly to `/tools/vdb/ingest`.
- Remove any local parsing logic (handled server-side).

**Example Snippet:**
```python
col_secure, col_upload, col_send, col_clear = st.columns([1, 1, 3, 1])

with col_secure:
    st.session_state.secure_mode = st.toggle("Secure", value=st.session_state.secure_mode)

with col_upload:
    uploaded_file = st.file_uploader("ğŸ“„ Upload", type=["pdf", "txt", "docx", "md"], label_visibility="collapsed")
    if uploaded_file:
        headers = {"Authorization": f"Bearer {st.session_state.api_token}"}
        files = {"file": (uploaded_file.name, uploaded_file, uploaded_file.type)}
        with st.spinner(f"Uploading {uploaded_file.name}..."):
            res = requests.post(f"{API_BASE}/tools/vdb/ingest", headers=headers, files=files)
            if res.status_code == 200:
                st.success(f"âœ… {uploaded_file.name} uploaded to knowledge base")
            else:
                st.error(f"âš ï¸ Upload failed: {res.text[:100]}")
```

---

### 2ï¸âƒ£ Backend API â€” File Upload Endpoint

**File:** `app/api/tools.py`

**New Endpoint:**
```python
from fastapi import APIRouter, Depends, File, UploadFile, HTTPException
from app.security.auth import require_bearer
from app.tools.vdb import VDBAdapter
import logging

logger = logging.getLogger(__name__)
router = APIRouter()
vdb = VDBAdapter()

@router.post("/vdb/ingest")
async def ingest_file(file: UploadFile = File(...), user=Depends(require_bearer)):
    '''
    Upload and ingest document into the vector database.
    Supports: .pdf, .docx, .md, .txt
    '''
    try:
        file_bytes = await file.read()
        vdb.ingest_file(file.filename, file_bytes)
        return {"ok": True, "filename": file.filename}
    except Exception as e:
        logger.error(f"Ingestion failed for {file.filename}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
```

---

### 3ï¸âƒ£ File Parser â€” Backend Text Extraction

**File:** `app/utils/file_parser.py` *(new)*

```python
from io import BytesIO
from PyPDF2 import PdfReader
from docx import Document
import markdown
import re

def extract_text(file_bytes: bytes, filename: str) -> list[dict]:
    '''
    Parse supported document types into list of text blocks with page info.
    Returns: [{"page": 1, "text": "..."}]
    '''
    filename = filename.lower()
    if filename.endswith(".pdf"):
        reader = PdfReader(BytesIO(file_bytes))
        return [{"page": i + 1, "text": page.extract_text() or ""} for i, page in enumerate(reader.pages)]
    elif filename.endswith(".docx"):
        doc = Document(BytesIO(file_bytes))
        return [{"page": 1, "text": "\n".join([p.text for p in doc.paragraphs])}]
    elif filename.endswith(".md"):
        text = BytesIO(file_bytes).read().decode("utf-8", errors="ignore")
        plain = re.sub(r"<[^>]+>", "", markdown.markdown(text))
        return [{"page": 1, "text": plain}]
    elif filename.endswith(".txt"):
        text = BytesIO(file_bytes).read().decode("utf-8", errors="ignore")
        return [{"page": 1, "text": text}]
    else:
        raise ValueError(f"Unsupported file format: {filename}")
```

---

### 4ï¸âƒ£ Text Splitter â€” Chunking Utility

**File:** `app/utils/text_splitter.py` *(new)*

```python
def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> list[str]:
    '''
    Split text into overlapping chunks for embedding storage.
    '''
    text = text.replace("\n", " ").strip()
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks
```

---

### 5ï¸âƒ£ VDB Adapter â€” Backend Ingestion Logic

**File:** `app/tools/vdb.py`

```python
from typing import List, Dict, Any
from app.memory.vector_store import VectorStore
from app.utils.config import KNOWLEDGE_PATH
from app.utils.text_splitter import chunk_text
from app.utils.file_parser import extract_text
import logging

logger = logging.getLogger(__name__)

class VDBAdapter:
    description = "Search and manage knowledge documents using vector embeddings"

    def __init__(self):
        self.store = VectorStore(path=KNOWLEDGE_PATH, collection="knowledge_base")

    def ingest_file(self, filename: str, file_bytes: bytes):
        '''Parse, chunk, and store document in VDB'''
        pages = extract_text(file_bytes, filename)
        items = []
        for page_data in pages:
            page = page_data["page"]
            text = page_data["text"]
            for i, chunk in enumerate(chunk_text(text)):
                items.append({
                    "id": f"{filename}_p{page}_c{i}",
                    "text": chunk,
                    "metadata": {"filename": filename, "page": page, "chunk": i}
                })
        if not items:
            raise ValueError("No valid text extracted from document.")
        self.store.ingest(items)
        logger.info(f"Ingested {len(items)} chunks from {filename}")

    def query(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        results = self.store.search(query, top_k)
        return results
```

---

### 6ï¸âƒ£ Agent Core â€” Append Source Citations

**File:** `app/agent/core.py`

```python
def _plan_and_execute(self, user_id, text, intents, context, session_id):
    result = {
        "type": "answer",
        "answer": final_answer,
        "steps": executed_steps,
        "used_tools": used_tools,
        "citations": [],
    }

    vdb_results = [
        tool for tool in used_tools
        if tool.get("name", "").lower() in ("vdb", "vdbadapter", "vector_db")
    ]
    if vdb_results:
        citations = []
        for vtool in vdb_results:
            outputs = vtool.get("outputs", {})
            if isinstance(outputs, dict) and "results" in outputs:
                for r in outputs["results"]:
                    meta = r.get("metadata", {})
                    filename = meta.get("filename")
                    page = meta.get("page")
                    if filename:
                        citations.append({"source": filename, "page": page or "?"})
        if citations:
            unique = {f"{c['source']} (page {c['page']})" for c in citations}
            result["citations"] = citations
            result["answer"] += "\n\nğŸ“š **Sources:**\n" + "\n".join(f"- {s}" for s in unique)

    return result
```

---

## âœ… End-to-End Behavior

| Step | Action | Result |
|------|--------|---------|
| 1ï¸âƒ£ | User uploads PDF | Backend extracts text, chunks it, and stores embeddings in VDB |
| 2ï¸âƒ£ | User queries Agent | Agent retrieves from VDB |
| 3ï¸âƒ£ | Agent replies | Includes content + file references |
| 4ï¸âƒ£ | Secure Mode on | Guardrails applied to input/output sanitization |

**Sample Output:**
```
Federated learning allows multiple clients to train collaboratively without sharing raw data.

ğŸ“š **Sources:**
- privacy_ai.pdf (page 2)
- ml_fundamentals.docx (page 1)
```

---

## ğŸ§ª Testing Checklist

| Test | Expected Result |
|------|-----------------|
| Upload `.pdf` file | âœ… "Ingested into knowledge base" |
| Upload `.txt` / `.md` / `.docx` | âœ… Extracted + chunked correctly |
| Ask about uploaded doc | âœ… Agent retrieves and cites source |
| Secure Mode enabled | âœ… Input/output filtered by guardrails |
| Session change | âœ… Uploaded docs still retrievable |
| Upload unsupported file | âš ï¸ "Unsupported file format" |

---

## ğŸš€ Summary

After this feature, your Agentic AI MVP will support:
- Safer input/output via `secure_mode`
- Seamless knowledge ingestion from user documents
- Persistent, source-aware knowledge retrieval
- Modular backend for future RAG or multi-user expansion
